{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2384157e",
   "metadata": {},
   "source": [
    "1. Transformerì™€ ë¹„êµí•´ ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„\n",
    "- GPT-1ì—ëŠ” ì¸ì½”ë”ê°€ í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤. (ì¸ì½”ë” ì…ì¶œë ¥ ë“± ì¸ì½”ë”ì™€ ê´€ë ¨ëœ ë¶€ë¶„ ì‚­ì œ)\n",
    "    \n",
    "    `class EncoderLayer, class Encoder, class Transformer ì°¸ê³ `\n",
    "\n",
    "\n",
    "- Decoder ë¶€ë¶„ì—ì„œ Encoder-Decoder Attention ë¶€ë¶„ì´ ì—†ë‹¤. (Decoderì˜ ì²«ë²ˆì§¸ Attentionì—ì„œ ë‘ë²ˆì§¸ Attentionì„ ê±´ë„ˆë›°ì–´ì•¼ë¨)\n",
    "    \n",
    "    `class DecoderLayer, class Decoder ì°¸ê³ `\n",
    "\n",
    "\n",
    "- ì…ë ¥ ë°ì´í„°ì˜ í˜•ì‹ì—ì„œ Qì™€ Aë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ë„£ëŠ”ë‹¤.\n",
    "\n",
    "    `class ChatBotDataset ì°¸ê³ `\n",
    "\n",
    "- Positional Encoding ë°©ì‹ì´ ê³ ì •ëœ ìœ„ì¹˜ ì¸ì½”ë”©ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ë°”ë€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961764b",
   "metadata": {},
   "source": [
    "2. ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœì— ë§ê²Œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤.\n",
    "- GPT-1 ëª¨ë¸ì€ ë””ì½”ë”ë§Œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ì¸ì½”ë”ì˜ ì¸í’‹ì€ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "- ë””ì½”ë”ì˜ input : ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë…¼ë¬¸ì—ì„œ ìš”êµ¬í•˜ë“¯ì´ s Q + $[sep] + A /s í˜•íƒœë¡œ ì…ë ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤.\n",
    "\n",
    "    `class ChatBotDataset ì°¸ê³ `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91177458",
   "metadata": {},
   "source": [
    "3. ëª¨ë¸ì˜ ì…ë ¥ ë¸”ëŸ­ì„ GPT ë…¼ë¬¸ì— ê¸°ë°˜í•˜ì—¬ ìˆ˜ì •í•˜ì˜€ë‹¤.\n",
    "- ëª¨ë¸ì˜ iputì´ ì •ìƒì ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€, ë°ì´í„°ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì • êµ¬í˜„ í™•ì¸\n",
    "\n",
    "    `class Transformer, class Decoder, class DecoderLayer , class GPTPosionalEncoding ì°¸ê³ `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e5946",
   "metadata": {},
   "source": [
    "4. GPT ëª¨ë¸ì„ ì •ìƒì ìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.\n",
    "\n",
    "|![model.summary](model.summary.png)|![model.fit](model.fit.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bae304",
   "metadata": {},
   "source": [
    "5. ì…ë ¥ì— ë”°ë¥¸ ì¶œë ¥ì´ ìƒì„±ë˜ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e1b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c868661",
   "metadata": {},
   "source": [
    "STEP1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fecaad",
   "metadata": {},
   "source": [
    "STEP2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "- ì˜ì–´ ë°ì´í„°ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ë°ì´í„°ì¸ ë§Œí¼ ì˜ì–´ ë°ì´í„°ì— ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ì™€ ì¼ë¶€ ë™ì¼í•œ ì „ì²˜ë¦¬ë„ í•„ìš”í•˜ê² ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93508c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ì•ˆë…•í•˜ì„¸ìš” ! ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤ . , ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤ . !\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = \"C:/Users/Choi/Desktop/ChatbotData.csv\"\n",
    "MAX_SAMPLES = 300000\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "  # ì…ë ¥ë°›ì€ sentenceë¥¼ ì†Œë¬¸ìë¡œ ë³€ê²½í•˜ê³  ì–‘ìª½ ê³µë°±ì„ ì œê±°\n",
    "  sentence = sentence.lower().strip()\n",
    "\n",
    "  # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "  sentence = re.sub(r\"[^ã„±-ã…ê°€-í£0-9?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "sample_sentence = \"Hello? ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤.ğŸ˜Š, ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.!\"\n",
    "preprocessed_sentence = preprocess_sentence(sample_sentence)\n",
    "print(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f681cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë”©: C:/Users/Choi/Desktop/ChatbotData.csv\n",
      "ì „ì²´ ìƒ˜í”Œ ìˆ˜ : 11823\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë™ì‹œì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì„ ì „ì²˜ë¦¬\n",
    "def read_korean_chatbot_data(path_to_csv, max_samples=300000):\n",
    "    print(f\"ë°ì´í„° ë¡œë”©: {path_to_csv}\")\n",
    "    try:\n",
    "        # ChatbotData.csv íŒŒì¼ì€ 'utf-8' ì¸ì½”ë”©ì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "        df = pd.read_csv(path_to_csv, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # ë§Œì•½ utf-8 ì‹¤íŒ¨ ì‹œ 'cp949' ì‹œë„\n",
    "        df = pd.read_csv(path_to_csv, encoding='cp949')\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        q_text = preprocess_sentence(row['Q'])\n",
    "        a_text = preprocess_sentence(row['A'])\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ í›„ ë¹„ì–´ìˆì§€ ì•Šì€ ìŒë§Œ ì¶”ê°€\n",
    "        if q_text and a_text:\n",
    "            pairs.append((q_text, a_text))\n",
    "        \n",
    "        if len(pairs) >= max_samples:\n",
    "            print(f\"ìµœëŒ€ {max_samples}ê°œ ìƒ˜í”Œ ë¡œë”© ì™„ë£Œ.\")\n",
    "            break\n",
    "            \n",
    "    return pairs\n",
    "\n",
    "#ë¡œë“œí•œ ë°ì´í„°ì˜ ìƒ˜í”Œ ìˆ˜ í™•ì¸\n",
    "pairs = read_korean_chatbot_data(path_to_dataset, max_samples=MAX_SAMPLES)\n",
    "\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52392a",
   "metadata": {},
   "source": [
    "STEP3. SentencePiece ì‚¬ìš©í•˜ê¸°\n",
    "- í•œêµ­ì–´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤ê³  ë§ì€ ë¶„ì´ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì•„ë‹Œ ìœ„ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ì¸ SentencePieceë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1bb3eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ í›„ì˜ ë¬¸ì¥: ë‚˜ëŠ” ì„¼í…ìŠ¤í”¼ìŠ¤ë¡œ íŒŒì´í† ì¹˜ë¥¼ ë°°ìš°ê³  ìˆì–´ìš”\n",
      "Tokenized: ['â–ë‚˜ëŠ”', 'â–ì„¼', 'í…', 'ìŠ¤í”¼', 'ìŠ¤ë¡œ', 'â–íŒŒ', 'ì´', 'í† ', 'ì¹˜', 'ë¥¼', 'â–ë°°ìš°', 'ê³ ', 'â–ìˆì–´ìš”']\n",
      "Encoded: [688, 1858, 7136, 3600, 861, 860, 6788, 7406, 6960, 6845, 1072, 6797, 115]\n",
      "Decoded: ë‚˜ëŠ” ì„¼í…ìŠ¤í”¼ìŠ¤ë¡œ íŒŒì´í† ì¹˜ë¥¼ ë°°ìš°ê³  ìˆì–´ìš”\n"
     ]
    }
   ],
   "source": [
    "#1 Toklenizer í•™ìŠµí•˜ê¸°\n",
    "    #ìœ„ì—ì„œ ë§Œë“  pairë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥\n",
    "corpus_file = \"korean_chatbot_corpus.txt\"\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for q, a in pairs:\n",
    "        f.write(q + \"\\n\")\n",
    "        f.write(a + \"\\n\")\n",
    "\n",
    "#ì§ˆë¬¸-ë‹µë³€ ìŒì´ ë‹´ê¸´ íŒŒì¼ì„ ì´ìš©í•´ ëª¨ë¸ í›ˆë ¨í•˜ê³  íŒŒì¼ ìƒì„±\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=corpus_file,\n",
    "    model_prefix=\"spm_korean_chatbot\",\n",
    "    vocab_size=8000,\n",
    "    character_coverage=1.0,\n",
    "    model_type=\"bpe\",\n",
    "    max_sentence_length=999999,\n",
    "    bos_id=1,  # <s> (Beginning of Sentence) ì„¤ì •\n",
    "    eos_id=2,  # </s> (End of Sentence) ì„¤ì •\n",
    "    pad_id=0,  # Padding ID ì„¤ì •\n",
    "    unk_id=3   # Unknown Token ID ì„¤ì •\n",
    ")\n",
    "\n",
    "#í•™ìŠµ ì‹œí‚¨ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì™€ í…ŒìŠ¤íŠ¸\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"spm_korean_chatbot.model\")\n",
    "\n",
    "# ì˜ˆì œ ë¬¸ì¥\n",
    "sentence = \"ë‚˜ëŠ” ì„¼í…ìŠ¤í”¼ìŠ¤ë¡œ íŒŒì´í† ì¹˜ë¥¼ ë°°ìš°ê³  ìˆì–´ìš”\"\n",
    "\n",
    "sentence = preprocess_sentence(sentence)\n",
    "print(\"ì „ì²˜ë¦¬ í›„ì˜ ë¬¸ì¥:\", sentence)\n",
    "\n",
    "# 1. í† í¬ë‚˜ì´ì§• (subword ë‹¨ìœ„ë¡œ ë¶„í• )\n",
    "tokens = sp.encode(sentence, out_type=str)\n",
    "print(\"Tokenized:\", tokens)\n",
    "\n",
    "# 2. ì¸ì½”ë”© (ì„œë¸Œì›Œë“œë¥¼ ì •ìˆ˜ IDë¡œ ë³€í™˜)\n",
    "encoded = sp.encode(sentence, out_type=int)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# 3. ë””ì½”ë”© (ì •ìˆ˜ ID â†’ ì›ë³¸ ë¬¸ì¥ ë³µì›)\n",
    "decoded = sp.decode(encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50b5b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12ì‹œ ë•¡ !  â‡  í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš” .\n",
      "tensor([5550, 6827, 3198,  108, 6785,    3, 4480,  214, 5917,    4,    2,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "12ì‹œ ë•¡ !  â‡  í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš” .\n"
     ]
    }
   ],
   "source": [
    "#2 Dataset êµ¬í˜„í•˜ê¸°\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, pairs, sp, max_length=40):\n",
    "        super().__init__()\n",
    "        self.sp = sp\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        pad_id = sp.pad_id() # 0\n",
    "        bos_id = sp.bos_id() # 1\n",
    "        eos_id = sp.eos_id() # 2\n",
    "\n",
    "        for q_text, a_text in pairs:\n",
    "            # Qì™€ Aë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ë§Œë“¤ê¸°\n",
    "            combined = f\"{q_text} [SEP] {a_text}\"\n",
    "            ids = sp.EncodeAsIds(combined)\n",
    "\n",
    "            # ì‹œì‘ê³¼ ë ì¶”ê°€\n",
    "            tokens = [bos_id] + ids + [eos_id]\n",
    "\n",
    "            # 3) ê¸¸ì´ ì œí•œ\n",
    "            if len(tokens) > max_length:\n",
    "                continue\n",
    "\n",
    "            # 4) ê³ ì • ê¸¸ì´ íŒ¨ë”©\n",
    "            pad = [pad_id] * (max_length - len(tokens))\n",
    "            tokens = tokens + pad\n",
    "\n",
    "            # 5) ë””ì½”ë” ì…ë ¥(input): tokens[:-1], íƒ€ê²Ÿ(outputs): tokens[1:]\n",
    "            input_ids = tokens[:-1]\n",
    "            target_ids = tokens[1:]\n",
    "\n",
    "            self.data.append({\n",
    "                \"input\": input_ids,\n",
    "                \"target\": target_ids\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        input_ids = torch.tensor(sample[\"input\"], dtype=torch.long)\n",
    "        target_ids = torch.tensor(sample[\"target\"], dtype=torch.long)\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "dataset = ChatbotDataset(pairs, sp, max_length=40)\n",
    "\n",
    "\n",
    "# Decoding ê³¼ì •ì—ì„œ start, end í† í°ì€ ìƒëŸ‰í•˜ë‹ˆ ì˜ ì°¸ê³  startëŠ”1, endëŠ” 2ì˜ ì¸ë±ìŠ¤\n",
    "for decoder_input, decoder_label  in dataset:\n",
    "    #print(\"í…ì„œ í¬ê¸° :\",encoder_input.size())\n",
    "    #print(encoder_input)\n",
    "    #print(sp.decode(encoder_input.tolist()))\n",
    "    #print(decoder_input)\n",
    "    print(sp.decode(decoder_input.tolist()))\n",
    "    print(decoder_label)\n",
    "    print(sp.decode(decoder_label.tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b07f2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 DataLoader êµ¬ì„±í•˜ê¸°\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "#for encoder_input, decoder_input, decoder_label in dataloader:\n",
    "#    print(encoder_input.size())\n",
    "#    print(decoder_input.size())\n",
    "#    print(decoder_label.size())\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5c13e",
   "metadata": {},
   "source": [
    "STEP4. ëª¨ë¸ êµ¬ì„±í•˜ê¸° (íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ êµ¬í˜„)\n",
    "- ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f972e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.position = position\n",
    "        self.pos_encoding = self._build_pos_encoding(position, d_model)\n",
    "\n",
    "    def _get_angles(self, position, i, d_model):\n",
    "        return 1.0 / (10000.0 ** ((2.0 * (i // 2)) / d_model)) * position\n",
    "\n",
    "    def _build_pos_encoding(self, position, d_model):\n",
    "        pos = torch.arange(position, dtype=torch.float32).unsqueeze(1)\n",
    "        i = torch.arange(d_model, dtype=torch.float32).unsqueeze(0)\n",
    "        angle_rads = self._get_angles(pos, i, d_model)\n",
    "        sines = torch.sin(angle_rads[:, 0::2])\n",
    "        cosines = torch.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = torch.zeros(position, d_model)\n",
    "        pos_encoding[:, 0::2] = sines\n",
    "        pos_encoding[:, 1::2] = cosines\n",
    "\n",
    "        pos_encoding = pos_encoding.unsqueeze(0) \n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b1aa5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT-1ì„ ìœ„í•œ Positional Encoding class\n",
    "class GPTPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        return x + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "815ca800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
    "    depth = key.size(-1)  \n",
    "    logits = matmul_qk / math.sqrt(depth)\n",
    "    if mask is not None:\n",
    "        logits = logits + (mask * -1e9)\n",
    "    attention_weights = F.softmax(logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3975ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        self.query_dense = nn.Linear(d_model, d_model)\n",
    "        self.key_dense = nn.Linear(d_model, d_model)\n",
    "        self.value_dense = nn.Linear(d_model, d_model)\n",
    "        self.out_dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        x = x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Q, K, Vì— ê°ê° Linear ì ìš©\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # Head ë¶„í• \n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # ë‹¤ì‹œ (batch_size, seq_len, d_model)ë¡œ í•©ì¹˜ê¸°\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # ìµœì¢… Dense\n",
    "        output = self.out_dense(concat_attention)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "377bf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    # x == 0 ìœ„ì¹˜ë¥¼ ì°¾ì•„ floatí˜• 1ë¡œ ë³€í™˜\n",
    "    mask = (x == 0).float()\n",
    "    # (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "044a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = x.size(1)\n",
    "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len)))\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    look_ahead_mask = look_ahead_mask.unsqueeze(0)\n",
    "    look_ahead_mask = look_ahead_mask.unsqueeze(1)\n",
    "    look_ahead_mask = look_ahead_mask.to(x.device)\n",
    "    combined_mask = torch.max(look_ahead_mask, padding_mask)\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f09b5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # ì´ì „ì— êµ¬í˜„í•œ MHA\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # í”¼ë“œí¬ì›Œë“œ ë¶€ë¶„ (Dense -> ReLU -> Dense)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # (1) ë©€í‹° í—¤ë“œ ì–´í…ì…˜ (ì…€í”„ ì–´í…ì…˜)\n",
    "        attn_output = self.mha(x, x, x, mask)  # (batch_size, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(x + attn_output)     # ì”ì°¨ ì—°ê²° + LayerNorm\n",
    "\n",
    "        # (2) í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§\n",
    "        ffn_output = self.ffn(out1)            # (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.norm2(out1 + ffn_output)   # ì”ì°¨ ì—°ê²° + LayerNorm\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc7aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # (1) ì„ë² ë”© ë ˆì´ì–´\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # (2) í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # (3) EncoderLayer ìŒ“ê¸°\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # (1) ì„ë² ë”© & sqrt(d_model)ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        # (2) í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš© + ë“œë¡­ì•„ì›ƒ\n",
    "        x = self.pos_encoding(x)  # shape: (batch_size, seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # (3) num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° EncoderLayer í†µê³¼\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91753313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        #self.encdec_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        #self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),  # Dense(units=ff_dim)\n",
    "            nn.ReLU(),                   # activation='relu'\n",
    "            nn.Linear(ff_dim, d_model)   # Dense(units=d_model)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, look_ahead_mask=None):\n",
    "        self_attn_out = self.self_mha(x, x, x, mask=look_ahead_mask)\n",
    "        out1 = self.norm1(x + self.dropout(self_attn_out))  # ì”ì°¨ ì—°ê²° + LayerNorm\n",
    "\n",
    "        #encdec_attn_out = self.encdec_mha(out1, enc_outputs, enc_outputs, mask=padding_mask)\n",
    "        #encdec_attn_out = self.dropout2(encdec_attn_out)\n",
    "        #out2 = self.norm2(out1 + encdec_attn_out)  # ì”ì°¨ ì—°ê²° + LayerNorm\n",
    "\n",
    "        ffn_out = self.ffn(out1)\n",
    "        out3 = self.norm3(out1 + self.dropout(ffn_out))  # ì”ì°¨ ì—°ê²° + LayerNorm\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cf42f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = GPTPositionalEncoding(max_len=512, d_model=d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, look_ahead_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)  \n",
    "        x = self.dropout(x)\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, look_ahead_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b259d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì•ì„œ ì‚¬ìš©í•œ ì¸ì½”ë” ì¸µ, ë””ì½”ë” ì¸µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ í•¨ìˆ˜ ì •ì˜\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,      # ì¸ì½”ë”/ë””ì½”ë” ì¸µ ìˆ˜\n",
    "                 units,           # feed-forward ë„¤íŠ¸ì›Œí¬ì˜ ì¤‘ê°„ ì°¨ì›(ff_dim)\n",
    "                 d_model,         # ì„ë² ë”© ë° ë‚´ë¶€ í‘œí˜„ ì°¨ì›\n",
    "                 num_heads,       # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ í—¤ë“œ ìˆ˜\n",
    "                 dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # ì¸ì½”ë”\n",
    "        #self.encoder = Encoder(\n",
    "        #    vocab_size=vocab_size,\n",
    "        #    num_layers=num_layers,\n",
    "        #    ff_dim=units,\n",
    "        #    d_model=d_model,\n",
    "        #    num_heads=num_heads,\n",
    "        #    dropout=dropout\n",
    "        #)\n",
    "\n",
    "        # ë””ì½”ë”\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            num_layers=num_layers,\n",
    "            ff_dim=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # ìµœì¢… ì¶œë ¥ì¸µ: (d_model) -> (vocab_size)\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, dec_inputs):\n",
    "        #enc_padding_mask = create_padding_mask(inputs)     \n",
    "        look_ahead_mask = create_look_ahead_mask(dec_inputs) \n",
    "        #dec_padding_mask = create_padding_mask(inputs)     \n",
    "\n",
    "        #enc_outputs = self.encoder(\n",
    "        #    x=inputs,\n",
    "        #    mask=enc_padding_mask\n",
    "        #)  \n",
    "\n",
    "        dec_outputs = self.decoder(\n",
    "            x=dec_inputs,          \n",
    "            #enc_outputs=None, #enc_outputs,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            #padding_mask=None   # dec_padding_mask\n",
    "        )  \n",
    "\n",
    "        # 6) ìµœì¢… Dense (vocab_size)\n",
    "        logits = self.final_linear(dec_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5afd2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(8000, 256)\n",
      "    (pos_encoding): GPTPositionalEncoding(\n",
      "      (pos_embedding): Embedding(512, 256)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (dec_layers): ModuleList(\n",
      "      (0-1): 2 x DecoderLayer(\n",
      "        (self_mha): MultiHeadAttention(\n",
      "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_linear): Linear(in_features=256, out_features=8000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#1 ëª¨ë¸ ìƒì„±\n",
    "# ì˜ˆ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "NUM_LAYERS = 2     # ì¸ì½”ë”/ë””ì½”ë” ì¸µ ìˆ˜\n",
    "D_MODEL = 256      # ì„ë² ë”© ë° ë‚´ë¶€ í‘œí˜„ ì°¨ì›\n",
    "NUM_HEADS = 8      # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜\n",
    "UNITS = 512        # í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ ì°¨ì›\n",
    "DROPOUT = 0.1      # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "VOCAB_SIZE = 8000 # ë‹¨ì–´ ì§‘í•© í¬ê¸°(ì˜ˆì‹œ)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "#2 ì†ì‹¤í•¨ìˆ˜\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "\n",
    "\n",
    "#3 ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ \n",
    "def get_lr_lambda(d_model, warmup_steps=4000):\n",
    "    d_model = float(d_model)\n",
    "    def lr_lambda(step):\n",
    "        # stepì€ 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1ë¡œ ë³´ì •\n",
    "        step = step + 1\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "    return lr_lambda\n",
    "\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "d_model = 512\n",
    "warmup_steps = 4000\n",
    "total_steps = 200000  # ì´ í•™ìŠµ ìŠ¤í…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27bb88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 ëª¨ë¸ ì»´íŒŒì¼\n",
    "# Optimizer ì •ì˜\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Scheduler ì •ì˜\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000))\n",
    "\n",
    "def accuracy_function(y_pred, y_true, pad_id=0):\n",
    "    preds = y_pred.argmax(dim=-1)  \n",
    "    mask = (y_true != pad_id)\n",
    "    correct = (preds == y_true) & mask\n",
    "    acc = correct.float().sum() / mask.float().sum()\n",
    "    return acc\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cfe7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 í›ˆë ¨í•˜ê¸°\n",
    "def train_step(model, batch, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    #enc_input, dec_input, target = [x.to(device) for x in batch]\n",
    "    dec_input, target = [x.to(device) for x in batch]\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ëª¨ë¸ í¬ì›Œë“œ íŒ¨ìŠ¤\n",
    "    #logits = model(enc_input, dec_input) \n",
    "    logits = model(dec_input) \n",
    "\n",
    "    # Loss ê³„ì‚° (íŒ¨ë”© í† í° ë¬´ì‹œ)\n",
    "    #loss = loss_function(logits.permute(0, 2, 1), target)  \n",
    "    loss = loss_function(logits.transpose(1, 2), target)  \n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), accuracy_function(logits, target, pad_id=sp.pad_id())\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, loss_function, scheduler, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "\n",
    "            # ì¼ì • ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[Epoch {epoch+1}, Step {step}] Loss: {loss:.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Completed - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d2ac68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step 0] Loss: 9.1403, Acc: 0.0000\n",
      "[Epoch 1, Step 100] Loss: 9.1331, Acc: 0.0023\n",
      "[Epoch 1, Step 200] Loss: 9.1108, Acc: 0.0000\n",
      "[Epoch 1, Step 300] Loss: 9.1025, Acc: 0.0000\n",
      "Epoch 1 Completed - Avg Loss: 9.1354, Avg Acc: 0.0001\n",
      "[Epoch 2, Step 0] Loss: 9.1790, Acc: 0.0000\n",
      "[Epoch 2, Step 100] Loss: 9.1052, Acc: 0.0000\n",
      "[Epoch 2, Step 200] Loss: 9.1368, Acc: 0.0000\n",
      "[Epoch 2, Step 300] Loss: 9.0782, Acc: 0.0000\n",
      "Epoch 2 Completed - Avg Loss: 9.1180, Avg Acc: 0.0001\n",
      "[Epoch 3, Step 0] Loss: 9.0877, Acc: 0.0000\n",
      "[Epoch 3, Step 100] Loss: 9.0553, Acc: 0.0000\n",
      "[Epoch 3, Step 200] Loss: 9.0486, Acc: 0.0000\n",
      "[Epoch 3, Step 300] Loss: 9.0746, Acc: 0.0000\n",
      "Epoch 3 Completed - Avg Loss: 9.0797, Avg Acc: 0.0001\n",
      "[Epoch 4, Step 0] Loss: 9.0403, Acc: 0.0000\n",
      "[Epoch 4, Step 100] Loss: 9.0634, Acc: 0.0000\n",
      "[Epoch 4, Step 200] Loss: 9.0302, Acc: 0.0000\n",
      "[Epoch 4, Step 300] Loss: 8.9945, Acc: 0.0000\n",
      "Epoch 4 Completed - Avg Loss: 9.0246, Avg Acc: 0.0001\n",
      "[Epoch 5, Step 0] Loss: 8.9968, Acc: 0.0000\n",
      "[Epoch 5, Step 100] Loss: 8.9612, Acc: 0.0000\n",
      "[Epoch 5, Step 200] Loss: 8.9571, Acc: 0.0023\n",
      "[Epoch 5, Step 300] Loss: 8.9251, Acc: 0.0000\n",
      "Epoch 5 Completed - Avg Loss: 8.9498, Avg Acc: 0.0002\n",
      "CPU times: total: 2min 24s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=5,  # ì›í•˜ëŠ” ì—í­ ìˆ˜\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc640fb9",
   "metadata": {},
   "source": [
    "STEP5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
    "- Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac85ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì˜ˆì¸¡ì„ ìœ„í•œ Decoder_inference í•¨ìˆ˜ì •ì˜\n",
    "\n",
    "def decoder_inference(model, sentence, tokenizer, device='cpu'):\n",
    "    START_TOKEN = tokenizer.bos_id()\n",
    "    END_TOKEN = tokenizer.eos_id()\n",
    "    MAX_LENGTH = 40\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    #enc_input_ids = [START_TOKEN] + tokenizer.encode(sentence) + [END_TOKEN]\n",
    "    #enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
    "    dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long, device=device)\n",
    "    model.eval()  # ëª¨ë¸ í‰ê°€ ëª¨ë“œ\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH):\n",
    "            #logits = model(enc_input, dec_input)\n",
    "            logits = model(dec_input)\n",
    "            predicted_id = torch.argmax(logits[:, -1, :], dim=-1) \n",
    "            if predicted_id.item() == END_TOKEN:\n",
    "                break\n",
    "            dec_input = torch.cat([dec_input, predicted_id.unsqueeze(0)], dim=1)\n",
    "    output_sequence = dec_input.squeeze(0).tolist()  \n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "\n",
    "#ì±—ë´‡ì˜ ëŒ€ë‹µì„ ì–»ëŠ” í•¨ìˆ˜\n",
    "def sentence_generation(model, sentence, tokenizer, device='cpu'):\n",
    "    output_seq = decoder_inference(model, sentence, tokenizer, device=device)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [token for token in output_seq if token < tokenizer.GetPieceSize()]\n",
    "    )\n",
    "\n",
    "    print(\"ì…ë ¥ :\", sentence)\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ddc9dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì–´ë”” ìˆì—ˆì–´?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë¶€ë‹´ìŠ¤ëŸ½ê¸´ ì—  ì´í•´ ì‹ ê²½ì“°ëŠ” ì˜ˆë¹„ ë¶ˆê¸ˆ ê°€ëŠ”ê²Œ ê¸°ìˆ ì—ˆê² ë„¤ìš” ìƒ¤ì›Œ ëŠë¼ ìì¥ë©´ ì—¬ìì¹œêµ¬ì™€ ê°€ëŠ¥í•´ìƒìƒì´ì—ìš”ê¶ˆ ê·¸ëŒ€ë˜ìš”ë‹¤ë¥¸ ìˆì„ê¹Œìš” ì„ íƒ í˜ ë¡œë§¨í‹±í•˜ë„¤ìš”í™ˆ ìˆìŒ ì ê·¹ì ìœ¼ë¡œ ì‹¶ì§€ ê¾¸ë©° ë‚˜ì•„ì¡Œí•´ì¤˜ì„œ ë‚¨ê²¨ í˜ ë¡œë§¨í‹±í•˜ë„¤ìš” ë­”ê°€ ë°˜ì¥ ì¬ë¯¸ìˆê²Œí¥ ìƒìƒ'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'ì–´ë”” ìˆì—ˆì–´?'\n",
    "sentence_generation(model, sentence, sp, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd3c781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì´ê±´ í•¨ì •ì´ì•¼\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë¶€ë‹´ìŠ¤ëŸ½ê¸´ ì—  ì´í•´ ì‹ ê²½ì“°ëŠ” ì˜ˆë¹„ ë¶ˆê¸ˆ ê°€ëŠ”ê²Œ ê¸°ìˆ ì—ˆê² ë„¤ìš” ìƒ¤ì›Œ ëŠë¼ ìì¥ë©´ ì—¬ìì¹œêµ¬ì™€ ê°€ëŠ¥í•´ìƒìƒì´ì—ìš”ê¶ˆ ê·¸ëŒ€ë˜ìš”ë‹¤ë¥¸ ìˆì„ê¹Œìš” ì„ íƒ í˜ ë¡œë§¨í‹±í•˜ë„¤ìš”í™ˆ ìˆìŒ ì ê·¹ì ìœ¼ë¡œ ì‹¶ì§€ ê¾¸ë©° ë‚˜ì•„ì¡Œí•´ì¤˜ì„œ ë‚¨ê²¨ í˜ ë¡œë§¨í‹±í•˜ë„¤ìš” ë­”ê°€ ë°˜ì¥ ì¬ë¯¸ìˆê²Œí¥ ìƒìƒ'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"ì´ê±´ í•¨ì •ì´ì•¼\"\n",
    "sentence_generation(model, sentence, sp, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
